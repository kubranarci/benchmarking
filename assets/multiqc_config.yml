report_comment: >
  This report has been generated by the <a href="https://github.com/nf-core/variantbenchmarking/tree/dev" target="_blank">nf-core/variantbenchmarking</a>
  analysis pipeline. For information about how to interpret these results, please see the
  <a href="https://nf-co.re/variantbenchmarking/dev/docs/output" target="_blank">documentation</a>.
report_section_order:
  "nf-core-variantbenchmarking-methods-description":
    order: -1000
  software_versions:
    order: -1001
  "nf-core-variantbenchmarking-summary":
    order: -1002

export_plots: true

disable_version_detection: true

# Run only these modules
run_modules:
  - happy
  - sompy
  - rtgtools
  - svanalyzer
  - truvari
  - wittyer
  - merge_reports
  - plots

module_order:
  - happy:
      name: "hap.py benchmarking"
      info: " metrics generated by hap.py tool - per sample"
  - sompy:
      name: "som.py benchmarking"
      info: " metrics generated by som.py tool - per sample"
  - rtgtools:
      name: "RTGtools eval benchmarking"
      info: " metrics generated by RTGtools eval tool - per sample"
  - svanalyzer:
      name: "SVANALYZER benchmarking"
      info: " metrics generated by svbenchmark tool - per sample"
  - truvari:
      name: "truvari benchmarking"
      info: " metrics generated by truvari tool - per sample"
  - wittyer:
      name: "witty.er benchmarking"
      info: " metrics generated by witty.er tool - per sample"
  - merge_reports:
      name: "Summary tables"
  - plots:
      name: "Summary plots"

sp:
  truvari:
    fn: "*.summary.json"
